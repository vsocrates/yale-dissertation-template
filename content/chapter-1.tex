\chapter{Uncertainty Quantification in a Real-World Clinical Task: Deprescribing} \label{chapter:deprescribing}

\section{Introduction}
The aim of this thesis is to assess uncertainty quantification (UQ) and, in doing so, evaluate the Bayesian diagnostic reasoning capabilities of large language models (LLMs). A fundamental requirement for Bayesian diagnostic reasoning is a comprehensive understanding and quantification of clinical uncertainty. Clinical judgment frequently demands both qualitative and quantitative interpretations of uncertainty \cite{one of the papers from David list recently}. For instance, the LLM must be able to quantify the uncertainty associated with a differential list of diagnoses to determine the most useful diagnostic test to run next, similar to how a physician would reason about a patient during diagnostic wayfinding. However, in a similar scenario, the relative risks associated with a particular patient situation may convince a physician to choose a less sensitive diagnostic test with lower risk. For instance, a particular patient may be afraid to get an CT for a head injury given their lack of experience with radiation imaging, despite it being the most sensitive to determine TBI. Therefore, the physician may need to perform an US instead, despite the quantitative risk suggesting an MRI. Therefore, while the quantitative interpretation of uncertainty may be relevant in determining the "ideal" decision making process, qualitative elements influence decision making. These sometimes competing priorities contribute to the art and science of medicine and require clinical gestalt. An LLM that attempts to align with current clinical practice must handle both these qualitative and quantitative interpretations of uncertainty. In this work, we tackle the former through an inherently qualitative uncertainty quantification task: the process of recommendation deprescribing in older adults (i.e. $\geq$ 65 years old). 

Deprescribing is defined as the systematic process of identifying and discontinuing drugs, called potentially inappropriate medications (PIMs), whose present or potential harms outweigh benefits provided to the patient within the context of their individual care goals and quality of life \citep{hohlPolypharmacyAdverseDrugrelated2001a, scottReducingInappropriatePolypharmacy2015}. This process is primarily conducted in patients that are at-risk for drug-related negative outcomes: those with polypharmacy regimes. Widely defined as the regular use of at least five medications, polypharmacy is common in older adults and at-risk populations\citep{halli-tierneyPolypharmacyEvaluatingRisks2019}. In fact, approximately 30\% of patients aged 65 years or older have polypharmacy\citep{scottReducingInappropriatePolypharmacy2015}, and nearly half of older emergency department (ED) patients are discharged with one or more new medications\cite{skainsGeriatricEmergencyMedication2024}. Although necessary and beneficial for some patients, polypharmacy can increase risk of negative consequences for patients, including emergency department (ED) visits, adverse drug events (ADEs), falls, disability, and inappropriate medication use\cite{halli-tierneyPolypharmacyEvaluatingRisks2019}.

Deprescribing tools, such as the Screening Tool of Older People’s Prescriptions (STOPP) and Beers criteria, have been developed to help providers assess and identify PIMs based on a patient’s medication list\citep{candeiasPotentiallyInappropriateMedications2021,bythe2023americangeriatricssocietybeerscriteriarupdateexpertpanelAmericanGeriatricsSociety2023,kaufmannInappropriatePrescribingSystematic2014}. These explicit assessments are criterion-based with clear standards, but are often impractical to implement in time-constrained clinical settings, such as the emergency department\citep{leeChallengesOpportunitiesCreating2022}. Attempts to digitize these criteria into electronic clinical decision support have raised difficulties, typically requiring a labor-intensive coding process and unstructured information from patient records to contextualize certain criteria\citep{anrysSTOPPSTARTVersion2016b,scottUsingEMRenabledComputerized2018}. Large language models (LLMs) have been shown to interpret complex clinical situations and offer recommendations, from differential diagnoses to care management, leading to growing interest in their application in the medical field\citep{clusmannFutureLandscapeLarge2023, gilsonHowDoesChatGPT2023, kungPerformanceChatGPTUSMLE2023, savageDiagnosticReasoningPrompts2024}. Moreover, they have been shown to extract medication-related data such as medication name, dosage, and frequency, necessary for application of deprescribing criteria\citep{goelLLMsAccelerateAnnotation2023}. Lastly, LLMs are excellent in-context learners, requiring very little labeled data to make predictions\citep{agrawal-etal-2022-large} reducing the annotation burden for time-constrained EM physicians while improving the use of unstructured patient records to contextualize patient medication lists. However, the majority of clinical reasoning evaluations on LLMs have been conducted using standardized exams (USMLE) or online case reports\citep{savageDiagnosticReasoningPrompts2024,savageLargeLanguageModel2024}. Their ability to perform clinical reasoning and calibrate responses over physician-generated text remains unclear. 

In this project, we propose to evaluate the performance of an end-to-end LLM-based pipeline in recommending deprescribing options for ED patients at discharge based on explicit deprescribing criteria, such as the Beers and STOPP criteria. This work will help address gaps in electronic deprescribing by using an LLM to contextualize recommendations within individual patient records and reduce manual development in CDS tools. In doing so, we hope to further the widespread adoption of electronic deprescribing in clinical practice.

\section{Methods}
\section{Results}
\section{Discussion}
\section{Relevance to Initial Hypothesis}
