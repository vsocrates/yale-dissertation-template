\chapter{Uncertainty Quantification in a Real-World Clinical Task: Deprescribing} \label{chapter:deprescribing}

\section{Introduction}
The aim of this thesis is to assess uncertainty quantification (UQ) and, in doing so, evaluate the Bayesian diagnostic reasoning capabilities of large language models (LLMs). A fundamental requirement for Bayesian diagnostic reasoning is a comprehensive understanding and quantification of clinical uncertainty. Clinical judgment frequently demands both qualitative and quantitative interpretations of uncertainty \cite{one of the papers from David list recently}. For instance, the LLM must be able to quantify the uncertainty associated with a differential list of diagnoses to determine the most useful diagnostic test to run next, similar to how a physician would reason about a patient during diagnostic wayfinding. However, in a similar scenario, the relative risks associated with a particular patient situation may convince a physician to choose a less sensitive diagnostic test with lower risk. For instance, a particular patient may be afraid to get an CT for a head injury given their lack of experience with radiation imaging, despite it being the most sensitive to determine TBI. Therefore, the physician may need to perform an US instead, despite the quantitative risk suggesting an MRI. Therefore, while the quantitative interpretation of uncertainty may be relevant in determining the "ideal" decision making process, qualitative elements influence decision making. These sometimes competing priorities contribute to the art and science of medicine and require clinical gestalt. An LLM that attempts to align with current clinical practice must handle both these qualitative and quantitative interpretations of uncertainty. In this work, we tackle the former through an inherently qualitative uncertainty quantification task: the process of recommendation deprescribing in older adults (i.e. $\geq$ 65 years old). 

Deprescribing is defined as the systematic process of identifying and discontinuing drugs, called potentially inappropriate medications (PIMs), whose present or potential harms outweigh benefits provided to the patient within the context of their individual care goals and quality of life \citep{hohlPolypharmacyAdverseDrugrelated2001a, scottReducingInappropriatePolypharmacy2015}. This process is primarily conducted in patients that are at-risk for drug-related negative outcomes: those with polypharmacy regimes. Widely defined as the regular use of at least five medications, polypharmacy is common in older adults and at-risk populations\citep{halli-tierneyPolypharmacyEvaluatingRisks2019}. In fact, approximately 30\% of patients aged 65 years or older have polypharmacy\citep{scottReducingInappropriatePolypharmacy2015}, and nearly half of older emergency department (ED) patients are discharged with one or more new medications\cite{skainsGeriatricEmergencyMedication2024}. Although necessary and beneficial for some patients, polypharmacy can increase risk of negative consequences for patients, including emergency department (ED) visits, adverse drug events (ADEs), falls, disability, and inappropriate medication use\cite{halli-tierneyPolypharmacyEvaluatingRisks2019}.

Deprescribing tools, such as the Screening Tool of Older People’s Prescriptions (STOPP) and Beers criteria, have been developed to help providers assess and identify PIMs based on a patient’s medication list\citep{candeiasPotentiallyInappropriateMedications2021,bythe2023americangeriatricssocietybeerscriteriarupdateexpertpanelAmericanGeriatricsSociety2023,kaufmannInappropriatePrescribingSystematic2014}. These explicit assessments are criterion-based with clear standards, but are often impractical to implement in time-constrained clinical settings, such as the emergency department\citep{leeChallengesOpportunitiesCreating2022}. Attempts to digitize these criteria into electronic clinical decision support have raised difficulties, typically requiring a labor-intensive coding process and unstructured information from patient records to contextualize certain criteria\citep{anrysSTOPPSTARTVersion2016b,scottUsingEMRenabledComputerized2018}. Large language models (LLMs) have been shown to interpret complex clinical situations and offer recommendations, from differential diagnoses to care management, leading to growing interest in their application in the medical field\citep{clusmannFutureLandscapeLarge2023, gilsonHowDoesChatGPT2023, kungPerformanceChatGPTUSMLE2023, savageDiagnosticReasoningPrompts2024}. Moreover, they have been shown to extract medication-related data such as medication name, dosage, and frequency, necessary for application of deprescribing criteria\citep{goelLLMsAccelerateAnnotation2023}. Lastly, LLMs are excellent in-context learners, requiring very little labeled data to make predictions\citep{agrawal-etal-2022-large} reducing the annotation burden for time-constrained EM physicians while improving the use of unstructured patient records to contextualize patient medication lists. However, the majority of clinical reasoning evaluations on LLMs have been conducted using standardized exams (USMLE) or online case reports\citep{savageDiagnosticReasoningPrompts2024, savageLargeLanguageModel2024}. Both of these exam types are multiple choice and require clinical gestalt and reasoning to make decisions under  qualitative clinical uncertainty. In some cases, patient considerations need to be taken into account. In most, explicit quantification of benefits and risks is unnecessary, and instead relative risks are weighed against one another, in combination with guidelines and rules of thumb. However, LLMs ability to perform this form of qualitative reasoning over physician-generated text (such as clinical notes) remains unclear.  

In this chapter, we propose to evaluate the performance of an end-to-end LLM-based pipeline in recommending deprescribing options for ED patients at discharge based on explicit deprescribing criteria. In addition to helping address gaps in electronic deprescribing by using an LLM to reduce manual development in CDS tools, this work will  help elucidate if LLMs have an internal model of qualitative uncertainty that aligns with that of physicians and medical students. We also investigate if we can use this model to improve their performance using selective prediction methods, allowing for a more collaborative physician-AI system.



\section{Methods}

We describe the methods used to evaluate a qualitative uncertainty quantification task, namely, deprescribing  medications in older adults. 

\subsection{Patient Cohort}

Our cohort consists of all older adults ($\geq$ 65 yo.) patients with polypharmacy ($\geq$ 5 active outpatient medications) presenting to the Yale New Haven-Health Emergency Department between January-March 2022 totaling 10,977 patients across 15,161 encounters. We select a random, convenience sample of 100 unique patients with 898 medications based on budget constraints and power analysis based on the unique medications across all patients (\textbf{Need to put in the power analysis here: XXX}). On average, a patient in our cohort had ~9 medications and all were considered separately by both the LLM and annotators, given the same patient information for filtering and recommendation. 

\subsection{Consensus-Based High-Yield Criteria Evaluation}

In an informal pilot study, one of the main causes of discrepancies between physicians and LLMs arose from ambiguous inclusion/exclusion conditions in deprescribing criteria (e.g. "Statins for primary cardiovascular prevention in persons aged $\geq$ 85 and established frailty with expected life expectancy likely less than 3 years."). Both established frailty and expected life expectancy are difficult to quantify and therefore implement. As a result, we first conducted a rigorous consensus-based evaluation of deprescribing criteria from three different recommendation lists: STOPP, Beers, and GEMS-Rx. 

To identify high-yield criteria likely to be amenable to automated review, we evaluated a total of 180 recommendations across two dimensions: Clinical Applicability and EHR Computability, under the assumption that high-yield criteria must both be highly clinically applicable (e.g. pose high risk to the patient, be feasible for deprescribing in various clinical contexts) and identifiable within the EHR. The consensus panel consisted of 6 board-certified physicians (in Emergency Medicine, Internal Medicine, Cardiology, Med-Peds, Geriatrics) and 1 ED pharmacist at YNHH. 

Each member of the group individually reviewed each of the criteria and rated them on a 5-point Likert scale for each of 5 questions of clinical applicability and 4 questions of EHR computability as shown in Figure A1. In our final selection, we averaged all EHR computability responses across panelists and selected only clinical risk to the patient to represent Clinical Applicability. Due to the high number of potentially high-yield criteria, we then selected the top 50\% criteria in terms of EHR computability and risk to patients arriving at 81 total criteria across all three deprescribing lists. A plot of EHR computability and patient risk is shown in Figure 2. While we captured information on deprescribing feasibility in various contexts in our consensus study for further qualitative evaluation, it was not included in selection of high-yield criteria as they should be relevant irrespective of the clinical context under which they may be feasible. 


\subsection{Deprescribing Recommendations by GPT-4o}

Given the cognitive burden required to evaluate the large list of criteria, we hypothesize that LLMs will be more effective than clinical experts in identifying relevant clinical criteria, but worse at applying them given the ambiguous nature of even high-yield inclusion/exclusion conditions. To determine an LLM?s efficacy on these two questions, as well as to reduce the context size provided to the model, we developed a 2-step pipeline, as shown in Figure 1. In step 1, GPT-4o is prompted to filter the full list of high-yield criteria solely based on the patient?s medication list, ignoring inclusion/exclusion conditions. This both reduces confusion due to large input context sizes and ensures that extraneous context doesn?t distract the LLM. In step 2, GPT-4o is prompted to use its previously filtered criteria list, along with structured (e.g. demographics, lab values, vitals, and PMH) and unstructured (most recent progress note and discharge summary) information, to determine if the patient satisfies any deprescribing criteria and therefore should be recommended for deprescribing. Aside from the sampling-based method, described below, all LLM calls are performed with almost no variation (\emph{temperature}=0, set seed). 

\subsection{Selective Prediction}

For both steps, we also collect GPT-4o?s confidence associated with its decisions using two confidence elicitation methods. The LLM?s confidence will be used to determine if the model should abstain due to low certainty regarding its own decision. In practice, this case would be considered too difficult for the LLM and forwarded to an expert reviewer. This human-in-the-loop decision making pipeline is known as selective prediction and has been commonly found to improve performance in non-text-based applications. In this work, we evaluate whether LLMs? seemingly well-calibrated confidence on question-answering tasks such as the USMLE transfer to a real-world clinical recommendation task. 

We select validated, effective versions of the two main confidence elicitation methods: chain-of-thought-driven verbalized confidence and self-random sampling with average-confidence aggregation. For the prompt-based method, we ask the LLM to explicitly estimate its confidence for both steps following its decision. For the sampling-based method, we repeat the sample question with high temperature (T=0.8) several (N=5) times and use a verbalized confidence-weighted confidence estimate. We evaluate both selective prediction methods using risk-coverage curves, substituting coverage for LLM deferring fraction. 

\subsection{Comparison and Adjudication with Clinical Experts}

We compare GPT-4o?s filtered criteria lists and recommendations against human clinical experts to determine the LLM?s performance. Due to the low IRR found in our pilot study, we elect to have two senior (M4) medical students evaluate all medications in the test cohort and then have discrepancies between med student and the LLM adjudicated by two senior, board-certified ED physicians. In other words, we do not assume that medical students are the gold standard. For each medication, a medical student would  determine (1) if there exists a relevant high-yield criteria based on the medication list, and (2) whether the medication should be recommended for deprescribing. Of the 898 total medications, 75 were repeated by both medical students to validate the low inter-rater reliability (IRR) between junior clinical experts. We then compute discrepancies between medical students and the LLM and provide them to the ED physicians to determine who was correct and in cases where the LLM was incorrect, why this was the case. We leverage a prior evaluation framework to determine LLM error modes. We measure the IRR (Cohen?s k: Eligibility?0.619, Deprescribing?0.764) between the two senior ED physicians to ensure that coding practices were standardized prior to adjudication on the full set of GPT-medical student discrepancies. A discussion of the low k for eligibility is explained in the discussion. 


\section{Results}

As discovered in our pilot study, the medical student inter-rater reliability (IRR) in both steps was low (Cohen?s k: step 1-0.741 , step 2-0.082) across 75 medications. This led to an exploration of their performance compared to GPT-4o. As shown in Figure 3, the majority of medications (~56.1\%) were not included in the high-yield deprescribing criteria. However, there were a number of discrepancies that needed to be adjudicated. Of these, 90 (10.9\%) were lower priority as they determined eligibility of a medication, but either the med student or the GPT model ultimately decided its recommendation for deprescribing was not necessary. However, the majority (135 - 16.4\%) could potentially lead to a change in medication management. A significant source of discrepancy comes from the LLM?s significantly higher likelihood to recommend deprescribing (14.5\%) compared to the medical students (6.3\%). 

Upon adjudication by senior physicians, we found that across all discrepancies, \textbf{XXX} performed better than \textbf{XXX}. When looking at the error modes that led to GPT errors, when filtering criteria, adjudicators found that \textbf{XXX} was most common, followed by \textbf{XXX}. Conversely, in making recommendations to deprescribe, \textbf{XXX} was most common. 
Finally, we investigated whether eliciting confidence estimates from the LLM and using them to determine if the model should abstain would improve results. We compare both verbalized confidence with sampling-based confidence to find that sampling-based confidence outperformed verbalized when filtering criteria (max F1-Score: 0.811), but verbalized confidence performed better when making deprescribing recommendations (max F1-Score: 0.145) as shown in Figures 4 and 5. Additionally, regardless of method, selective prediction improved performance in eligibility as the deferring fraction increased (confidence threshold increased), while the inverse was true when making deprescribing recommendations, albeit with a low F1-score to begin with. In general, \textbf{XXX} are able to determine criteria-eligible medication better, while \textbf{XXX} are able to make deprescribing recommendations better. Moreover, selective prediction is not effective across the board, but is task-dependent. 


\section{Discussion}
\section{Relevance to Initial Hypothesis}
