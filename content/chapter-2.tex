\chapter{LLM Biases in Bayesian Diagnostic Reasoning} \label{chapter:race-bayes}

\section{Introduction}
In Chapter~\ref{chapter:deprescribing}, we 

The interpretation of diagnostic testing is a central component of clinical decision making. Diagnostic testing is the single most performed medical activity, with more than half of all clinical decisions made on the basis of laboratory testing1,2. A clear understanding of test characteristics and the judicious ordering of tests is vital in both reducing healthcare expenditures and improving overall quality of care for patients3,4. Classical diagnostic reasoning involves updating initial estimates of disease likelihood as diagnostic testing is ordered and results are returned. This process can be modeled through Bayes' rule (Figure 1)5. Unfortunately, significant evidence suggests that physicians are inaccurate in their estimates of disease probabilities and test characteristics, often falling for cognitive biases6?10.
 
Large language models (LLMs) have been shown to effectively perform numerical reasoning in a variety of settings.11,12 A recent study examined GPT-4's ability to estimate disease probabilities via Bayes' rule across four patient vignettes, demonstrating that the LLM was comparable to physicians for estimating post-test probability after a positive diagnostic test and more accurate than physicians after negative diagnostic tests across these vignettes.6 From this study, however, it was challenging to determine where the LLM misstepped in its erroneous answers, be it in its estimation of sensitivity or specificity of the diagnostic test, versus in its application of Bayes' rule. Nor did it examine how variation in the prompt provided to the LLM?be it specifically instructing the LLM to apply Bayes' rule or the use of chain-of-thought reasoning13?may impact results. Finally, each disease was only evaluated as a single condition, limiting the conclusions that can be drawn on disease-dependent variation in LLM disease probability estimation.

In addition, despite LLMs' significant potential to improve medical practice, there is concern that they will further exacerbate health inequities due to hidden sociodemographic biases arising from training data.14,15 In particular, Zack et al. tested the impact of varying demographics on the generation of clinical vignettes, finding that for conditions with similar prevalence by race and gender, such as COVID-19, GPT-4 was significantly more likely to generate vignettes with men16. Similarly, Nastasi et al. found that ChatGPT recommended a community clinic for an uninsured patient, while recommending the ED for an insured patient, despite the need for emergency care17. Despite initial work done in evaluating the impact of LLMs' biases on clinical decision making, previous studies have not evaluated how these biases may impact diagnostics, such as in the context of a clinical decision support tool. 

Our work aims to expand on previous research by evaluating both disease variation and racial biases in estimating risk through a Bayesian diagnostic framework by two state-of-the-art LLMs of different reasoning capabilities: GPT 4o-mini and GPT 4o. By modifying the availability of likelihood ratio information, we seek to identify specific points in the reasoning process that influence risk estimation. To investigate potential biases in the diagnostic risk assessments of these LLMs, we evaluate several vignettes across 4 distinct conditions and systematically vary race throughout our evaluation.


\section{Methods}

\subsection{Vignette Selection}

To evaluate the impacts of disease and race on Bayesian reasoning in LLMs, we gathered clinical vignettes from four common ED presentations with clearly defined diagnostic tests and validated positive and negative likelihood ratios (LRs). Specifically, we chose previously developed  vignettes from Brush et al18. The vignettes included clinical presentations, history, physical exam findings, as well as vitals and lab values when relevant. A total of 43 vignettes were used, covering 4 ED diagnoses: acute coronary syndrome (ACS), congestive heart failure (CHF), pneumonia, and pulmonary embolism (PE). Each vignette was also accompanied by a diagnostic laboratory test or radiologic investigation that was conducted following initial presentation. 

Originally designed to assess Bayesian reasoning in medical students, each vignette begins by asking for an initial pre-test probability of disease on a scale from 0 to 100\%. After determining the pre-test probability, participants were given the results of a relevant diagnostic test, reported as either positive or negative, along with literature-validated data on sensitivity, specificity, and both positive (LR+) and negative (LR-) likelihood ratios. Finally, participants used the initial presentation and test results to estimate a post-test probability of disease. We adapted these vignettes to require the LLM to estimate sensitivity and specificity, rather than relying on literature-derived values from Brush et al., allowing for a more fine-grained evaluation of its reasoning process. We selected vignettes that were physiologically likely, regardless of race for our racial bias evaluation. A description of the vignettes is shown in Table 1. All initial presentations included the patient's age, gender, and race.

\subsection{Bias Dimensions}

LLMs have the potential to perpetuate context biases due to both disease domain and race. To assess the impact of disease-specific bias, we average over all vignettes associated with a specific disease in our evaluation. To assess the impact of racial bias on risk estimation, we vary race in the history of present illness (HPI) section of the vignette using the U.S. Department of the Interior racial categories: American Indian or Alaska Native, Asian, Black or African American, Hispanic or Latino, Native Hawaiian or Other Pacific Islander, and White.

\subsection{Bias-Aware Diagnostic Reasoning Evaluation}

We conducted all experiments through Azure OpenAI using both GPT 4o and GPT 4o-mini  employing prompts specifying chain-of-thought (CoT)19. CoT reasoning has been shown to significantly improve reasoning in LLMs, allowing us to test the full capabilities of both models. Given that LLMs are inherently stochastic generative models, we sampled the full distribution of disease probability estimates for a given vignette by setting a high temperature temperature (0.8, from [0-1]) and repeating each vignette 10 times with GPT 4o-mini and 5 times with GPT 4o for both positive and negative test results. The high temperature ensures that the LLM generates different responses for each trial, allowing us to characterize the full range of probability estimates an LLM might generate20.

We evaluate performance of both LLMs by computing the post-test probability error, defined as the difference between the true post-test probability, computed using the estimated pre-test probability and literature-derived likelihood ratios, and the estimated post-test probability generated from the LLM. The estimated pre-test probability is assumed to be valid, acknowledging that prevalence can depend on various factors such as geographic location and presenting signs or symptoms. Consequently, our focus is on evaluating the models' capacity for Bayesian reasoning rather than their ability to estimate initial clinical disease risk. We also compute the estimated likelihood ratios from estimated sensitivity and specificity, and compare these to the provided LRs in Brush et al. 

To estimate post-test probability, likelihood ratio information was provided to the models in 3 different ways. First, we evaluate implicit estimation of disease risk by estimating both pre-test probabilities and post-test probabilities following test results, without any likelihood ratio information or explicit reference to Bayesian reasoning. Next, the LLMs estimate sensitivity and specificity, given the initial presentation of a patient and the related diagnostic test to encourage Bayesian reasoning while still limiting external information. Next, the LLMs were tasked with predicting post-test risk, given their estimated LRs. Finally, we substitute the estimated LRs with the literature-specified LRs from Brush et al. as a baseline. This allowed evaluation of diagnostic reasoning given implicit, partial and full Bayesian information.

Finally, to examine racial bias, for each vignette, we alter the patient's race in the HPI. Given the repeated trials described above, this yields a total of 5,160 for GPT 4o-mini and 2,580 runs for GPT 4o. In addition to estimating disease probability distributions, we hypothesize that differences in diagnostic accuracy are inherent biases baked into the models from training data, not a function of their stochasticity, so conduct LR estimation in a mostly-deterministic setting (temperature=0.0; set random seed). 


\section{Results}

In Figure 1, we present the post-test probability error of GPT 4o-mini and GPT 4o with three elements of likelihood ratio information: none, estimated, and true LRs. Across all disease conditions, GPT 4o-mini estimated disease risk more accurately when given either estimated (post-test prob. error: -1.75 $\pm$ 19.69) or literature-derived LRs (0.38 $\pm$ 8.22), compared to when the model estimated risk with implicit Bayesian reasoning (4.01 $\pm$ 24.29). This trend was also evident in GPT 4o, but with even greater accuracy when provided literature-derived LRs (0.02 $\pm$ 0.52). Both models variably demonstrated under- or overestimation of disease risk by disease condition. For instance, both models significantly overestimated disease risk, regardless of Troponin results when no LRs were provided (GPT 4o-mini: -3.50 $\pm$ 32.72, GPT 4o: -14.66 $\pm$ 19.68).

We also show differences in estimated LRs by race, across two temperature values for both LLMs in Figure 2.  In general, all models, regardless of temperature, underestimated the diagnostic accuracy of chest X-rays in diagnosis of CHF, while overestimating the power of Troponin I in diagnosis of ACS. However, as compared to GPT 4o-mini, GPT 4o estimated LRs for D-dimer in diagnosis of PE more accurately. 


\section{Discussion}

In this study, we conducted a rigorous evaluation of potential disease and race biases in disease probability estimation by large language models, employing a Bayesian pre-test/post-test probability framework. Utilizing validated vignettes for 4 common ED diagnoses, we evaluated both GPT 4o and GPT 4o-mini LLMs using the Azure OpenAI API assessing their ability to estimate pre-test probabilities, likelihood ratios, and post-test probabilities, while systematically varying the ED diagnosis of interest and race. 

In general, our results showed that LLMs struggled to accurately estimate post-test probabilities when no diagnostic test characteristics were estimated or provided. However, when using estimated or literature-derived LRs, in almost all cases, LLMs were able to employ Bayes' rule to improve their estimation of post-test disease risk. In particular, ACS and pneumonia are estimated incorrectly more so than other conditions by both models, potentially due to the vignettes with these ED diagnoses being cases that could be easily confused, namely ACS vs. hypertensive emergency and pneumocystis pneumonia vs. influenza-like illness. 

To understand why post-test probability estimates vary so significantly by condition, we hypothesized that LLMs misestimate the diagnostic accuracy of tests differently across conditions, affecting their ability to estimate post-test probabilities. To test this, we compared literature-derived likelihood ratios (LRs) with LLM-estimated LRs. For both models, it appears that conditions where the LLM has a poor understanding of the diagnostic accuracy of a particular condition (e.g. positive and negative results for CHF and pneumonia in GPT-4o) led to worse diagnostic probability error rates. In contrast, LRs for D-dimer in diagnosing PE were accurately estimated leading to lower error rates for both positive and negative lab results. This suggests that proper understanding of the predictive power of diagnostic tests is instrumental in disease probability estimates by LLMs. 

When breaking down the evaluation by race, while there were differences in post-test probability error across all three types of LRs, they were not statistically significant. Upon qualitative error analysis, we find that GPT 4o-mini explicitly mentioned race in \textbf{XXX}\% of responses, while GPT 4o mentioned race in \textbf{XXX}\%. While ultimately not impacting probability estimations, underlying racial bias may need to be considered when using LLMs to estimate disease risk. 

In this work, we conducted a comprehensive evaluation of disease probability estimates from two state-of-the-art LLMs, identifying notable differences across disease conditions when implicit and partial Bayesian information was provided. These differences appear to stem from limitations in estimating the predictive power of diagnostic tests in both models. Encouragingly, error rates did not significantly differ by race, though race was mentioned in both models' reasoning processes. Our findings underscore the importance of detailed assessment of reasoning steps when using LLMs as clinical risk predictors, as performance may vary across disease contexts, each requiring independent evaluation.

\section{Relevance to Initial Hypothesis}
