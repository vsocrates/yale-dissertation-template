\chapter{Conclusion} \label{chapter:conclusion}

This thesis focuses on a rigorous evaluation of the diagnostic reasoning capabilities of large language models (LLMs) under uncertainty within the clinical sublanguage. As outlined in the Motivation (Ch.~\ref{section:motivation}), our work proposes an initial theory of LLMs' abilities to quantify clinical uncertainty and evaluates this theory across real-world clinical scenarios. The overarching goal is to address critical gaps in understanding how LLMs internalize uncertainty and how this influences their clinical decision-making processes.

\section{Summary of Contributions}

In this thesis, we presented investigations into the Uncertainty Quantification (UQ) capabilities of several black-box Large Language Models. To achieve this, we conducted investigations into the uncertainty quantification (UQ) capabilities of black-box LLMs across three distinct but interconnected studies. In Chapter~\ref{chapter:deprescribing}, we examined the internal model of uncertainty in LLMs through a deprescribing recommendation task for older adults. These initial experiments revealed how LLMs navigate qualitative uncertainty in clinical settings. From these experiments, we established a preliminary theory of clinical uncertainty within LLMs. Chapter~\ref{chapter:race-bayes} extended this analysis by assessing whether an LLM’s internal uncertainty could be quantitatively evaluated and integrated with an external Bayesian reasoning framework in a classic diagnostic task. Central to diagnostic decision-making is the process of updating risk estimates based on diagnostic test results, and our initial theory was tested in such a real-world setting. This chapter also examined potential biases in the model’s Bayesian reasoning, particularly in contexts involving race and disease. Finally, in Chapter~\ref{chapter:bn-reasoning}, we extend our simulation of a real-world diagnostic application, investigating the LLM’s reasoning in a conversational diagnostic task. Using chest pain cases in the emergency department, we evaluated whether incorporating explicit probabilistic information, such as likelihood ratios and disease prevalence, could improve the model’s diagnostic accuracy and decision-making process. Collectively, these chapters present a comprehensive progression: from internal model evaluation in an ambiguous use case, to integration with external reasoning frameworks, and ultimately to real-world diagnostic applications enhanced with probabilistic reasoning.

Across these chapters, we provide a detailed examination of the uncertainty quantification capabilities of LLMs and their implications in clinical settings. In Chapter 1, we found that while the LLM demonstrated some ability to express qualitative uncertainty, its internal model was poorly calibrated, failing to align with its actual diagnostic performance. The LLM was able to identify relevant medications from a large list of deprescribing criteria, which is to be expected as this is largely a filtering problem with limited uncertainty and clinical gestalt. However, when compared to medical students, LLMs performed worse in making deprescribing recommendations, which requires balancing the risks of stopping medications with the potential benefits of reduced ADEs. Moreover, the LLM was unable to distinguish between riskier and less risky medications to deprescribe. A potential cause for this discrepancy could be that when queried directly, the LLM estimates that up to 20-50\% of medications may be appropriate candidates for deprescribing. However, prior work in deprescribing has shown that the true number of candidates is much lower (10-15\%)\citep{scottReducingInappropriatePolypharmacy2015}. This disconnect between the true likelihood of intervention and the perceived rate highlights significant challenges in its ability to manage uncertainty effectively. Recent work on diagnostic uncertainty supports these findings\citep{huangFutureAIClinicians2024}, emphasizing the need for robust calibration in clinical AI systems.

In Chapter 2, we explored a diagnostic setting in which uncertainty estimates were required to be updated correctly under a Bayesian framework. Evaluating the influence of patient characteristics on the LLM's uncertainty quantification, we found no significant impacts of race on LLM Bayesian reasoning, potentially as a result of the significant work on AI alignment and bias mitigation that has been conducted on LLMs\citep{gallegosBiasFairnessLarge2024}. However, the LLM's internal uncertainty was heavily influenced by the disease context, as evidenced by its estimation of likelihood ratios (LRs). In particular, the predictive accuracy of chest x-rays in the diagnosis of congestive heart failure (CHF) and pneumonia is severely underestimated by the LLM, as compared to a D-dimer for pulmonary embolism. This in turn increased the Bayesian post-test probability error rates, resulting in reduced diagnostic accuracy for conditions where the LLM failed to adequately model the uncertainty resolved by specific diagnostic tests. This demonstrates that the model’s representation of uncertainty is highly context-dependent, with substantial variation in performance due to specific diseases. Together, these two chapters suggest that an LLM's clinical uncertainty quantification is contingent on disease contexts. Moreover, they both support the view that an LLM's internal model does not align with either real world decision making or a Bayesian framework.
 
We further evaluate this theory in Chapter \ref{chapter:bn-reasoning}, which investigated the influence of disease contexts in a conversational diagnostic task, representing the diagnostic narrowing process often performed by physicians. The LLM was tasked with autonomously diagnosing chest pain patients presenting to the ED, aiming to investigate whether incorporating explicit probabilistic information, such as likelihood ratios and disease prevalence, could improve its diagnostic reasoning and overall performance. By varying the prompts to either remove restrictions on the diseases that the LLM could select or incorporate prevalence information, we observed significant reductions in false positives ($\Delta=+28\%$, $\Delta=+21\%$, respectively). However, the LLM continued to overestimate real-world disease rates, indicating a persistent misalignment between its internal uncertainty model and actual clinical probabilities. This occurred regardless of whether prevalence information was included or not, underscoring limitations in its probabilistic reasoning.

Overall, these findings demonstrate and reinforce our theory that LLMs fail to align their internal representations of clinical uncertainty with the complexities of real-world practice, despite including external probabilistic information at inference time. This discrepancy appears to be largely driven by clinical context and the LLM's limited understanding of clinical probabilities, a shortcoming that becomes particularly evident when its performance is compared to that of physicians. These limitations underscore the need for caution in deploying LLMs in clinical decision-making, whether in human-in-the-loop systems to assist physicians or autonomously as medical AI chatbots. Robust strategies for improving uncertainty modeling are essential to ensure their safe and effective implementation in healthcare.

\section{Limitations and Generalizability}
In this work, we established a preliminary theory of uncertainty in black-box LLMs and conducted a rigorous evaluation of this framework across various clinical decision-making settings. However, several limitations and considerations regarding the model’s generalizability must be acknowledged. 

In all three experiments, we focused exclusively on black-box LLMs. This decision was driven by several considerations. Black-box LLMs have consistently demonstrated superior performance in a range of benchmarks and are increasingly being implemented in clinical decision support systems, making their rigorous evaluation essential. However, this focus introduces certain limitations. Firstly, we are limited in our ability to extrapolate our conclusions to white-box models, which offer greater transparency and adaptability. Additionally, the black-box approach precludes the use of explainability techniques that require access to the internal layers of the model, such as mechanistic interpretability. Consequently, our analysis was restricted to treating the LLM as a participant in a cognitive psychology study, inferring its internal model of uncertainty solely from its text-based outputs. While this approach offers valuable insights, it limits our ability to probe the underlying mechanisms driving the model's reasoning processes. Moreover, due to patient privacy concerns, we had to restricts our evaluations to only OpenAI-family models investigated with de-identified notes through Microsoft's Azure platform, further limiting the generalizability of our results. Despite these model limitations, we believe that our results represent a starting point upon which to further develop an understanding of LLM's internal models of clinical uncertainty and their impact on decision making in real-world settings. 

In addition to the limitations associated with the types of models evaluated, our studies were also constrained by several data-related limitations. Notably, our initial study focused on a specific subdomain of clinical uncertainty, primarily related to pharmacological decision-making. While this work served as a use case to establish an initial theory of uncertainty in LLMs, the narrow scope raises questions about the generalizability of this theory. For instance, it has been shown that LLMs exhibit variable question-answering performance depending on the clinical subspecialty involved \citep{thirunavukarasuTriallingLargeLanguage2023}. Therefore, the LLM's model of uncertainty observed in our pharmacology-focused study may not extend to other clinical domains, where it might demonstrate either enhanced or diminished capability in modeling uncertainty based on the specific complexities of those tasks. Similarly, in Chapter~\ref{chapter:race-bayes}, our analysis was constrained by the relatively small number of conditions available in the vignette dataset. This limitation restricted the scope of our conclusions regarding disease-specific variability in Bayesian reasoning by LLMs. Nevertheless, the four conditions studied exhibited significant variability, which appeared to be primarily influenced by the LLM's inconsistent understanding of diagnostic test characteristics across cases. While these findings suggest that similar trends may extend to other clinical conditions, further investigations with a broader range of conditions are necessary to confirm the generalizability of these results.

Despite the limitations associated with both the types of LLMs evaluated and the clinical subdomains used in each experiment, we believe that our overall theory of LLM clinical uncertainty holds. Each experiment was validated using rigorous statistical tests over well-powered sample sizes. Furthermore, other studies in the literature have demonstrated similar limitations to uncertainty quantification and interpretation by LLMs, including overconfidence and high rates of logical errors\citep{savageLargeLanguageModel2024, huangFutureAIClinicians2024}, further supporting the generalizability of our studies. Our studies represent a promising initial step toward advancing our understanding of LLMs' clinical uncertainty reasoning, yet significant opportunities remain for future exploration and development in this field.

\section{Future work}
While this work has advanced our understanding of the clinical uncertainty model in LLMs, there remains considerable scope for future research. As highlighted in the previous section, this work can be expanded to evaluate other black-box LLMs once they support privacy-preserving implementations. Moreover, white-box models have recently improved significantly in performance, with some clinical domain-specific models such as Meditron \citep{chenMEDITRON70BScalingMedical2023} outcompeting commercial models. As computational resources become increasingly available at local healthcare institutions, detailed exploration of these methods using mechanistic approaches could uncover the underlying causes of poor model uncertainty quantification and identify potential avenues for improvement. Finally, generating additional vignettes or expanding the scope of the work done in Chapter~\ref{chapter:deprescribing} to include a broader set of evaluation tasks with a lens on uncertainty, similar to the work done by \citet{savageLargeLanguageModel2024}, would provide further insights into potential areas of improvement. These extensions to the work done in this thesis would lead to additional theories of uncertainty and new hypotheses for exploration. However, advancing this field will also require the development of novel methodological approaches for integrating real-world probabilities into LLMs.

In particular, having found that current LLMs have a poor internal model of clinical uncertainty, and therefore struggle to align with physicians in joint clinical decision-making tasks, it is clear that real-world probabilities need to be integrated into an LLM's internal representation. The question becomes,\emph{ at which stage in LLM text processing must these probabilities be integrated}? There are several options to that end. Firstly, we have shown that integrating real-world probabilistic knowledge during in-context learning largely yields negative results. Therefore, if integration is performed at inference time, the LLM must query an external data source directly, rather than relying solely on preprocessed input text. A potential integration method is by using retrieval-augmented generation (RAG)\cite{chenBenchmarkingLargeLanguage2024}. In the RAG paradigm, a query engine retrieves relevant context from a large document set, which the LLM uses to generate informed responses to queries. However, one limitation with this approach is that the probabilistic knowledge must exist in text prior to querying, which may not necessarily be the case. For instance, the prevalence of a particular diagnosis within a given population at an institution is not published, but may be relevant in an LLM's diagnosis. In such a setting, it may be necessary to compute the prevalence of certain conditions. Leveraging external tools, as demonstrated with Toolformer \citep{schickToolformerLanguageModels2023}, may provide a more effective approach. For instance, if the LLM identifies a need to include the likelihood of a cough in pneumonia during response generation, it can utilize a tool to query a dataset and add this probabilistic information to its context. This approach has the advantage of computing information dynamically while still not requiring retraining of the LLM. The implementation of such a set of tools may encourage the LLM to align better with physician clinical uncertainty calibration. However, our work has shown that inference time-injection of probabilistic information may not be enough to recalibrate the LLM. 

To address this, methods for integrating probabilistic knowledge at training time could be explored. While pretraining a model from scratch is often infeasible due to cost and computational constraints, this may not be necessary for this specific use case. The limitation in clinical uncertainty management is not the LLM's general language understanding but rather its lack of accurate probabilistic information. Consequently, instruction tuning with a dataset tailored to epidemiological queries may offer an effective solution for recalibrating the LLM's internal model of uncertainty. For instance, a dataset of common epidemiological questions from cohort studies could be automatically generated and the true answers could be computed over a cohort of real-world patients. These generated questions and corresponding answers would form the basis of the instruction tuning dataset. This strategy has the potential to align the LLM more closely with real-world probabilities, thereby improving its ability to quantify and reason about clinical uncertainty.

\section{Conclusion}
In this chapter, we explored the implications of our studies on the uncertainty quantification capabilities of state-of-the-art black-box LLMs. Through three rigorous investigations, we examined the internal models of clinical uncertainty in LLMs and their impact on decision-making. By comparing LLM performance with that of physicians, we identified critical gaps and used these findings to propose an overarching theory of uncertainty. This work represents a foundational step toward the rigorous evaluation of LLMs in real-world clinical tasks, which are often ambiguous and require nuanced clinical judgment. While our work has limitations, particularly in the types of models studied and the range of clinical tasks evaluated, the theory we propose provides a framework that can be further tested and refined. We believe LLMs represent a paradigm shift in clinical care, with the potential to significantly reduce clinical errors, reduce physician burnout, and improve patient outcomes. However, this potential hinges on the need for rigorous evaluation and thoughtful integration into clinical workflows. Aligning LLMs with physicians' decision-making criteria, priorities, and overarching clinical reasoning is essential to enabling effective human-AI collaboration in healthcare. We hope that our contributions provide a strong foundation for future research and development in this critical area.